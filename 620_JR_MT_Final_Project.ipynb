{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DATA 620 - Web Analytics - Final Project</h2>\n",
    "<h3>Team : Mohamed Thasleem, Kalikul Zaman and Jeyaraman Ramalingam</h3>\n",
    "<h3>Introduction</h3>\n",
    "    \n",
    "Build an sentiment analysis from popular twitter account and analyze possible network analysis with multiple twitter accounts\n",
    "\n",
    "We considered the following leader twitter account and perform sentiment analysis, also to build network graph specific to Donald Trump\n",
    "<ol>\n",
    "  <li>Donald Trump</li>\n",
    "  <li>Emmanuel Macron</li>\n",
    "  <li>Angela Merkel</li>\n",
    "  <li>Narendra Modi</li>\n",
    "  <li>Boris Johnson</li>\n",
    "</ol>\n",
    "   \n",
    "<h3>Analysis Overview</h3>\n",
    "<ol>\n",
    "  <li>Loading the dataset: Load the data and import the libraries.</li>\n",
    "  <li>Data Preprocessing (Analysing missing data and removing redendant columns)</li>\n",
    "  <li>Visualising and counting sentiments of tweets for each airline</li>\n",
    "  <li>Wordcloud plots for positive and negative tweets to visualise most frequent words for each.</li>\n",
    "  <li>Analysing the reasons for tweets for each leader Sentiment analysis VANDER and text blob approach</li>\n",
    "  <li>Predicting the tweet sentiments with tweet text data and comapare with multiple classifer models</li>\n",
    "  <li>Calculating accuracies of the models</li>\n",
    "  <li>Buliding Network Graph for Trump</li>\n",
    "  <li>Conclusion</li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing the libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import tweepy \n",
    "import pandas as pd\n",
    "import sys\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import twitter\n",
    "import re\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS,ImageColorGenerator\n",
    "from PIL import Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from pandasql import sqldf\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Declaring Functions required for this Project</h3>\n",
    "\n",
    "get_tweets - API to fetch the tweets<br>\n",
    "tweet_to_words - convert tweets to words<br>\n",
    "generate_wordcloud - Wordcloud generation using cleaned text<br>\n",
    "clean_tweet_length - Perform cleansing on tweets to pick only letters<br>\n",
    "clean_dataframe - Perform cleansing to elimiate junk words<br>\n",
    "predict_accuracy - Calcualte the accuracy using multiple model classifier<br>\n",
    "vader_sentiment - Function to perform VANDER sentiment<br>\n",
    "textblob_sentiment - Function to perform textblob sentiment<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(username): \n",
    "        auth = tweepy.OAuthHandler('uiFPpvK0s4ZQ0w0fjA6Nc2hHD', 'x1k4C3rsck0F3Iasi37j1vzn2RGu4ceM1VL0ylvVOIW1thDQwa') \n",
    "        auth.set_access_token('71980324-6VL5UR3GwTRWdrhZYMsuFCZCOQOWs1mrYA2eKh2R6', '8i7ssms0dZTqFyCsJRG1ASMMwJnLKUpLYniATeNwU0Ovl') \n",
    "        api = tweepy.API(auth) \n",
    "        tfile = []\n",
    "        for tweet in tweepy.Cursor(api.user_timeline, screen_name = username).items():\n",
    "            tfile.append([username, tweet.id_str,tweet.source, tweet.created_at,tweet.retweet_count,tweet.favorite_count, tweet.text])\n",
    "        df=pd.DataFrame(tfile)\n",
    "        return df    \n",
    "\n",
    "def generate_wordcloud(words, mask):\n",
    "    word_cloud = WordCloud(width = 512, height = 512, background_color='white',max_words=3000,stopwords=STOPWORDS, mask=mask,contour_width=1, contour_color='navy' ).generate(words)\n",
    "    plt.figure(figsize=(14,10),facecolor = 'white', edgecolor='red')\n",
    "    plt.imshow(word_cloud,interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "\n",
    "def tweet_to_words(raw_tweet):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_tweet) \n",
    "    words = letters_only.lower().split()                             \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words if not w in stops] \n",
    "    return( \" \".join( meaningful_words ))\n",
    "    \n",
    "def clean_tweet_length(raw_tweet):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_tweet) \n",
    "    from nltk.corpus import stopwords\n",
    "    words = letters_only.lower().split()                             \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words if not w in stops] \n",
    "    return(len(meaningful_words))\n",
    "    \n",
    "\n",
    "def clean_dataframe(df):\n",
    "    #Data Preparation , cleanup the characters in tweet column\n",
    "    pattern1 = re.compile(\" ' # S % & ' ( ) * + , - . / : ; < = >  @ [ / ] ^ _ { | } ~\")\n",
    "    pattern2 = re.compile(\"@[A-Za-z0-9]+\") \n",
    "    pattern3 = re.compile(\"https?://[A-Za-z0-9./]+\")\n",
    "    \n",
    "    df_tweet = df['tweet']\n",
    "    df_clean = []\n",
    "    #Replace Symbols, UTF-8 Characters\n",
    "    for item in df_tweet:\n",
    "        tweet = re.sub(pattern1, \"\", str(item))   \n",
    "        tweet = re.sub(pattern2, \"\", tweet)\n",
    "        tweet = re.sub(pattern3, \"\", tweet)\n",
    "        tweet=  str(tweet).replace(u\"\\\\xe2\\\\x80\\\\x99\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\xe2\\\\x80\\\\xa6\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\xe2\\\\x80\\\\x9\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\xf0\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\x9f\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\x91\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\x87\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\xbb\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\xe2\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\xe3\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\x86\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\x80\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\xa6\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\x8f\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\xa4\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\xe0\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"b\\'RT\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"b\\\"RT\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\xb8\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\xa5\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\xd7\",\"\")\n",
    "        tweet=  str(tweet).replace(u\"\\\\xd8\",\"\")\n",
    "        df_clean.append(tweet)\n",
    "    \n",
    "    #Final Data Frame is ready\n",
    "    df_mod = pd.DataFrame(df_clean,columns = ['tweet'])\n",
    "    #Prepaer Clean Tweet and Calculate Length of Tweet\n",
    "    df_mod['clean_tweet']=df_mod['tweet'].apply(lambda x: tweet_to_words(x))\n",
    "    df_mod['Tweet_length']=df_mod['tweet'].apply(lambda x: clean_tweet_length(x))\n",
    "    return df_mod\n",
    "\n",
    "def predict_accuracy(person,df):\n",
    "    #Calculating Sentiment\n",
    "    sentiment = SentimentIntensityAnalyzer()\n",
    "    def sentiment(tweet):\n",
    "        analysis = TextBlob(tweet)\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 1\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1    \n",
    "    df['VSA'] = np.array([ sentiment(tweet) for tweet in df['tweet'] ])\n",
    "\n",
    "    df['sentiment']  = np.select(\n",
    "        [\n",
    "            df['VSA']==0, \n",
    "            df['VSA']>0,\n",
    "            df['VSA']<0\n",
    "        ], \n",
    "        [\n",
    "            'Neutral', \n",
    "            'Negative',\n",
    "            'Positive'\n",
    "        ], \n",
    "        default='Unknown'\n",
    "    )\n",
    "    train,test = train_test_split(df,test_size=0.2,random_state=42)\n",
    "    train_clean_tweet=[]\n",
    "    for tweet in train['clean_tweet']:\n",
    "        train_clean_tweet.append(tweet)\n",
    "    test_clean_tweet=[]\n",
    "    for tweet in test['clean_tweet']:\n",
    "        test_clean_tweet.append(tweet)\n",
    "\n",
    "    #Fit the Count Vectorizer on Training and Test data\n",
    "    v = CountVectorizer(analyzer = \"word\")\n",
    "    train_features= v.fit_transform(train_clean_tweet)\n",
    "    test_features=v.transform(test_clean_tweet)\n",
    "\n",
    "    #Create Classifiers for predicting the sentiment of tweet\n",
    "    Classifiers = [\n",
    "    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True,gamma='auto'),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=200),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB()]\n",
    "\n",
    "#Fit the Classifier for the training data and predict the outcome of Trump tweet\n",
    "\n",
    "    dense_features=train_features.toarray()\n",
    "    dense_test= test_features.toarray()\n",
    "    Accuracy=[]\n",
    "    Model=[]\n",
    "    accuracy_res=[]\n",
    "    for classifier in Classifiers:\n",
    "        try:\n",
    "            fit = classifier.fit(train_features,train['sentiment'])\n",
    "            pred = fit.predict(test_features)\n",
    "        except Exception:\n",
    "            fit = classifier.fit(dense_features,train['sentiment'])\n",
    "            pred = fit.predict(dense_test)\n",
    "        accuracy = accuracy_score(pred,test['sentiment'])\n",
    "        Accuracy.append(accuracy)\n",
    "        Model.append(classifier.__class__.__name__)\n",
    "        accuracy_res.append([person,classifier.__class__.__name__,str(accuracy)])\n",
    "    return accuracy_res    \n",
    "\n",
    "def vader_sentiment(person,df_trump_mod):\n",
    "    sentiment_res=[]\n",
    "    sentiment = SentimentIntensityAnalyzer()\n",
    "    def sentiment_analyzer_scores(text):\n",
    "        score = sentiment.polarity_scores(text)\n",
    "        lb = score['compound']\n",
    "        if lb >= 0.05:\n",
    "            return 1\n",
    "        elif (lb > -0.05) and (lb < 0.05):\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    df_trump_mod['VSA'] = np.array([ sentiment_analyzer_scores(tweet) for tweet in df_trump_mod['tweet'] ])\n",
    "    \n",
    "    pos_tweets = [ tweet for index, tweet in enumerate(df_trump_mod['tweet']) if df_trump_mod['VSA'][index] > 0]\n",
    "    neu_tweets = [ tweet for index, tweet in enumerate(df_trump_mod['tweet']) if df_trump_mod['VSA'][index] == 0]\n",
    "    neg_tweets = [ tweet for index, tweet in enumerate(df_trump_mod['tweet']) if df_trump_mod['VSA'][index] < 0]\n",
    "    \n",
    "    pos_pct=len(pos_tweets)*100/len(df_trump_mod['tweet'])\n",
    "    neu_pct=len(neu_tweets)*100/len(df_trump_mod['tweet'])\n",
    "    neg_pct=len(neg_tweets)*100/len(df_trump_mod['tweet'])\n",
    "    sentiment_res.append([person,\"Positive Percentage\",str(pos_pct)])\n",
    "    sentiment_res.append([person,\"Negative Percentage\",str(neg_pct)])\n",
    "    sentiment_res.append([person,\"Neutral Percentage\",str(neu_pct)])\n",
    "    return sentiment_res\n",
    "\n",
    "def textblob_sentiment(person,df_trump_mod):\n",
    "    sentiment_res=[]\n",
    "    sentiment = SentimentIntensityAnalyzer()\n",
    "    def sentiment(tweet):\n",
    "        analysis = TextBlob(tweet)\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 1\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    df_trump_mod['VSA'] = np.array([ sentiment(tweet) for tweet in df_trump_mod['tweet'] ])\n",
    "    df_trump_mod['sentiment']  = np.select(\n",
    "        [\n",
    "            df_trump_mod['VSA']==0, \n",
    "            df_trump_mod['VSA']>0,\n",
    "            df_trump_mod['VSA']<0\n",
    "        ], \n",
    "        [\n",
    "            'Neutral', \n",
    "            'Negative',\n",
    "            'Positive'\n",
    "        ], \n",
    "        default='Unknown'\n",
    "    )\n",
    "    \n",
    "    pos_tweets = [ tweet for index, tweet in enumerate(df_trump_mod['tweet']) if df_trump_mod['VSA'][index] > 0]\n",
    "    neu_tweets = [ tweet for index, tweet in enumerate(df_trump_mod['tweet']) if df_trump_mod['VSA'][index] == 0]\n",
    "    neg_tweets = [ tweet for index, tweet in enumerate(df_trump_mod['tweet']) if df_trump_mod['VSA'][index] < 0]\n",
    "    \n",
    "    pos_pct=len(pos_tweets)*100/len(df_trump_mod['tweet'])\n",
    "    neu_pct=len(neu_tweets)*100/len(df_trump_mod['tweet'])\n",
    "    neg_pct=len(neg_tweets)*100/len(df_trump_mod['tweet'])\n",
    "    sentiment_res.append([person,\"Positive Percentage\",str(pos_pct)])\n",
    "    sentiment_res.append([person,\"Negative Percentage\",str(neg_pct)])\n",
    "    sentiment_res.append([person,\"Neutral Percentage\",str(neu_pct)])\n",
    "    return sentiment_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Twitter data</h3> \n",
    "    \n",
    "Loading the required files, API Calls are made to fetch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to twitter API and Obtain Donald Trump Tweets    \n",
    "df_trump=get_tweets(\"@realDonaldTrump\")  \n",
    "df_boris=get_tweets(\"@BorisJohnson\")  \n",
    "df_modi=get_tweets(\"@narendramodi\")  \n",
    "df_macron=get_tweets(\"@EmmanuelMacron\")  \n",
    "df_merkel=get_tweets(\"@Queen_Europe\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_trump=pd.read_csv(\"C:/Users/aisha/Desktop/tweets/@realDonaldTrump_tweets_V1.csv\")        \n",
    "#df_boris=pd.read_csv(\"C:/Users/aisha/Desktop/tweets/@BorisJohnson_V1.csv\")\n",
    "#df_macron=pd.read_csv(\"C:/Users/aisha/Desktop/tweets/@EmmanuelMacron_V1.csv\")\n",
    "#df_modi=pd.read_csv(\"C:/Users/aisha/Desktop/tweets/@narendramodi_V1.csv\")\n",
    "#df_merkel=pd.read_csv(\"C:/Users/aisha/Desktop/tweets/@Queen_Europe_V1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Cleansing</h3>\n",
    "\n",
    "Renaming Column Names and Calling Cleanup function to clean the data for any UTF-8 characters/symbols/unnecessary texts in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trump=df_trump.rename(columns={df_trump.columns[0]: \"User Name\", df_trump.columns[1]: \"Tweet ID\",df_trump.columns[2]: \"Source\",df_trump.columns[3]: \"created_date\",df_trump.columns[4]: \"retweet_count\",df_trump.columns[5]: \"favourite_count\",df_trump.columns[6]: \"tweet\"})\n",
    "df_modi=df_modi.rename(columns={df_modi.columns[0]: \"User Name\", df_modi.columns[1]: \"Tweet ID\",df_modi.columns[2]: \"Source\",df_modi.columns[3]: \"created_date\",df_modi.columns[4]: \"retweet_count\",df_modi.columns[5]: \"favourite_count\",df_modi.columns[6]: \"tweet\"})\n",
    "df_boris=df_boris.rename(columns={df_boris.columns[0]: \"User Name\", df_boris.columns[1]: \"Tweet ID\",df_boris.columns[2]: \"Source\",df_boris.columns[3]: \"created_date\",df_boris.columns[4]: \"retweet_count\",df_boris.columns[5]: \"favourite_count\",df_boris.columns[6]: \"tweet\"})\n",
    "df_merkel=df_merkel.rename(columns={df_merkel.columns[0]: \"User Name\", df_merkel.columns[1]: \"Tweet ID\",df_merkel.columns[2]: \"Source\",df_merkel.columns[3]: \"created_date\",df_merkel.columns[4]: \"retweet_count\",df_merkel.columns[5]: \"favourite_count\",df_merkel.columns[6]: \"tweet\"})\n",
    "df_macron=df_macron.rename(columns={df_macron.columns[0]: \"User Name\", df_macron.columns[1]: \"Tweet ID\",df_macron.columns[2]: \"Source\",df_macron.columns[3]: \"created_date\",df_macron.columns[4]: \"retweet_count\",df_macron.columns[5]: \"favourite_count\",df_macron.columns[6]: \"tweet\"})\n",
    "\n",
    "df_trump_mod=clean_dataframe(df_trump)\n",
    "df_macron_mod=clean_dataframe(df_macron)\n",
    "df_merkel_mod=clean_dataframe(df_merkel)\n",
    "df_boris_mod=clean_dataframe(df_boris)\n",
    "df_modi_mod=clean_dataframe(df_modi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Model Classifier</h3>\n",
    "\n",
    "Calling Predict Accuracy function to calculate Accuracy for the different classifiers used in the project. In this example picked only Trump for comparing the performance of calssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_trump = predict_accuracy(\"Donald Trump\",df_trump_mod)\n",
    "accuracy_trump = pd.DataFrame(accuracy_trump)\n",
    "accuracy_trump=accuracy_trump.rename(columns={accuracy_trump.columns[0]: \"Person\", accuracy_trump.columns[1]: \"Model\",accuracy_trump.columns[2]: \"Percentage\"})\n",
    "accuracy_trump=accuracy_trump.astype({'Percentage': 'float32'})\n",
    "accuracy_trump['Percentage']=round(accuracy_trump['Percentage']*100,2) \n",
    "accuracy_trump\n",
    "#Plot the Accuracy of Classifiers in predicting the sentiment of Trump Tweet\n",
    "Index = accuracy_trump['Model']\n",
    "Accuracy = accuracy_trump['Percentage']\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.bar(Index,Accuracy,color=['#588da8', '#ccafaf', '#e58a8a','#ffe277', '#58b4ae', '#ffb367', '#ffe2bc'])\n",
    "plt.xticks(Index, rotation=90)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.title('Accuracies of Models', fontdict = {'fontsize' : 20})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Frequency of words</h3>\n",
    "\n",
    "Frequently Used Words by Donald Trump in his masked image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordCloud of Tweets\n",
    "from PIL import Image\n",
    "#display(Image.open(\"C:/Users/aisha/Dropbox/CUNY/github/BB_Pics/t1.jpg\")) \n",
    "mask= np.array(Image.open(\"C:/Users/aisha/Dropbox/CUNY/github/BB_Pics/t12.jpg\"))\n",
    "stopwords = set(STOPWORDS)\n",
    "words = \" \".join(review for review in df_trump_mod.tweet)\n",
    "cleaned_word = \" \".join([word for word in words.split()\n",
    "                            if 'http' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and not word.startswith('&')\n",
    "                                and not word.startswith('ha')\n",
    "                                and not word.startswith('wa')\n",
    "                                and not word.startswith('thi')\n",
    "                                and word != 'RT'\n",
    "                            ])\n",
    "\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color='white',contour_width=1, contour_color='steelblue',\n",
    "                      width=4000,\n",
    "                      height=2500\n",
    "                     ).generate(cleaned_word)\n",
    "plt.figure(1,figsize=(15, 12))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud(str(cleaned_word), mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Top 20 Words used by Donald Trump</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_20_most_common_words(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    #words.remove('rt')\n",
    "    #print(words)\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:20]\n",
    "    del count_dict[0:1]\n",
    "    del count_dict[4:5]\n",
    "    #print(count_dict)\n",
    "    #print(type(count_dict))\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    \n",
    "    x_pos = np.arange(len(words)) \n",
    "    plt.figure(figsize=(16, 10))\n",
    "    my_colors = [\"#ff5252\",\"#ff4081\",\"#e040fb\",\"#7c4dff\",\"#536dfe\",\"#448aff\",\"#40c4ff\",\"#18ffff\",\"#64ffda\",\"#69f0ae\",\"#b2ff59\",\"#eeff41\",\"#ffff00\",\"#ffd740\",\"#ffab40\",\"#ff6e40\",\"#5d4037\",\"#616161\",\"#455a64\"]\n",
    "    plt.bar(x_pos, counts,align='center',color=my_colors)\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.title('Trump - 20 most common words', fontdict = {'fontsize' : 20})\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Find Most common 20 words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_data = count_vectorizer.fit_transform(df_trump_mod['tweet'])\n",
    "plot_20_most_common_words(count_data, count_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Number of Favorite Tweets and Retweets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trend on Week Days with most number of tweets\n",
    "fav_max = np.max(df_trump['favourite_count'])\n",
    "rt_max  = np.max(df_trump['retweet_count'])\n",
    "df_plt_fav = pd.DataFrame(zip(list(df_trump.favourite_count),list(df_trump.created_date)))\n",
    "fav = df_trump[df_trump.favourite_count == fav_max].index[0]\n",
    "rt  = df_trump[df_trump.retweet_count == rt_max].index[0]\n",
    "\n",
    "print(\"The tweet with more likes is: \\n{}\".format(df_trump['tweet'][fav]))\n",
    "print(\"Number of likes: {}\".format(fav_max))\n",
    "\n",
    "print(\"The tweet with more retweets is: \\n{}\".format(df_trump['tweet'][rt]))\n",
    "print(\"Number of retweets: {}\".format(rt_max))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tweet Trends</h3>\n",
    "\n",
    "Plotting the tweet trends based on day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt_fav=df_plt_fav.rename(columns={df_plt_fav.columns[0]: \"Count\", df_plt_fav.columns[1]: \"Created_Date\"})\n",
    "df_plt_fav['Created_Date'] = pd.to_datetime(df_plt_fav['Created_Date'])\n",
    "df_plt_fav['day_of_week']=df_plt_fav['Created_Date'].dt.day_name()\n",
    "df_plt = df_plt_fav.groupby([\"day_of_week\"]).count()\n",
    "Index=df_plt.index\n",
    "Count=df_plt.Count\n",
    "labels=df_plt.index\n",
    "values=df_plt.Count\n",
    "explode = [0.1, 0.1, 0.1, 0.1,0.1,0.1,0.1]\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "my_colors1 = [\"#ffff00\",\"#ffd740\",\"#ffab40\",\"#ff6e40\",\"#5d4037\",\"#616161\",\"#455a64\"]\n",
    "\n",
    "plt.pie(values, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90,colors=my_colors1)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.title('Tweets by days', fontdict = {'fontsize' : 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sentiment Analysis</h2>\n",
    "\n",
    "Sentiment Analysis has been done using VADER and TEXTBLOB analysis\n",
    "\n",
    "<h3>Sentiment Analysis by VADER</h3>\n",
    "\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vader_res_trump = vader_sentiment(\"Donald Trump\",df_trump_mod)\n",
    "vader_res_boris = vader_sentiment(\"Boris Johnson\",df_boris_mod)\n",
    "vader_res_modi = vader_sentiment(\"Narendra Modi\",df_modi_mod)\n",
    "vader_res_merkel = vader_sentiment(\"Angela Merkel\",df_merkel_mod)\n",
    "vader_res_macron = vader_sentiment(\"Emmanuel Macron\",df_macron_mod)\n",
    "pos_pct=round(float(vader_res_trump[0][2]),2)\n",
    "neu_pct=round(float(vader_res_trump[1][2]),2)\n",
    "neg_pct=round(float(vader_res_trump[2][2]),2)\n",
    "Index = [1,2,3]\n",
    "tweet_pct = [pos_pct,neu_pct,neg_pct]\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.bar(Index,tweet_pct, color=['#5f9e6e', '#5975a4', '#b55d60'])\n",
    "plt.xticks(Index,['positive','neutral','negative'])\n",
    "plt.title('Sentiment Analysis by VADER - Mood Analysis', fontdict = {'fontsize' : 20})\n",
    "plt.ylabel('Mood Count')\n",
    "plt.xlabel('Mood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sentiment Analysis by TextBlob</h3>\n",
    "\n",
    "TextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tblob_res_trump = textblob_sentiment(\"Donald Trump\",df_trump_mod)\n",
    "tblob_res_macron = textblob_sentiment(\"Emmanuel Macron\",df_macron_mod)\n",
    "tblob_res_merkel = textblob_sentiment(\"Angela Merkel\",df_merkel_mod)\n",
    "tblob_res_modi = textblob_sentiment(\"Narendra Modi\",df_modi_mod)\n",
    "tblob_res_boris = textblob_sentiment(\"Boris Johnson\",df_boris_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_pct=round(float(tblob_res_trump[0][2]),2)\n",
    "neu_pct=round(float(tblob_res_trump[1][2]),2)\n",
    "neg_pct=round(float(tblob_res_trump[2][2]),2)\n",
    "Index = [1,2,3]\n",
    "tweet_pct = [pos_pct,neu_pct,neg_pct]\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.bar(Index,tweet_pct, color=['#5f9e6e', '#5975a4', '#b55d60'])\n",
    "plt.title('Sentiment Analysis by TEXTBLOB - Mood Analysis', fontdict = {'fontsize' : 20})\n",
    "plt.xticks(Index,['positive','neutral','negative'])\n",
    "plt.ylabel('Mood Count')\n",
    "plt.xlabel('Mood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Leader Sentiment Comparison - TEXTBLOB</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tblob_all_leaders=[]\n",
    "for item in tblob_res_boris:\n",
    "    tblob_all_leaders.append(list([item[0],item[1],item[2]]))\n",
    "for item in tblob_res_trump:\n",
    "    tblob_all_leaders.append(list([item[0],item[1],item[2]]))\n",
    "for item in tblob_res_macron:\n",
    "    tblob_all_leaders.append(list([item[0],item[1],item[2]]))\n",
    "for item in tblob_res_merkel:\n",
    "    tblob_all_leaders.append(list([item[0],item[1],item[2]]))\n",
    "for item in tblob_res_modi:\n",
    "    tblob_all_leaders.append(list([item[0],item[1],item[2]]))\n",
    "\n",
    "tblob_all_df=pd.DataFrame(tblob_all_leaders)\n",
    "tblob_all_df.columns = ['Leader', 'Mood','Percentage']\n",
    "print(tblob_all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper')\n",
    "plt.figure(figsize=(16, 10))\n",
    "my_pal = ['#5f9e6e', '#b55d60', '#5975a4']\n",
    "# create plot\n",
    "sns.barplot(x = 'Leader', y = 'Percentage', hue = 'Mood', data = tblob_all_df,\n",
    "            palette = my_pal,\n",
    "            capsize = 0.05,             \n",
    "            saturation = 8,             \n",
    "            errcolor = 'gray', errwidth = 2,  \n",
    "            ci = 'sd'   \n",
    "            )\n",
    "plt.title('Leader Sentiment Comparison - TEXTBLOB', fontdict = {'fontsize' : 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Leader Sentiment Comparison - VADER</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vader_all_leaders=[]\n",
    "for item in vader_res_boris:\n",
    "    vader_all_leaders.append(list([item[0],item[1],item[2]]))\n",
    "for item in vader_res_trump:\n",
    "    vader_all_leaders.append(list([item[0],item[1],item[2]]))\n",
    "for item in vader_res_macron:\n",
    "    vader_all_leaders.append(list([item[0],item[1],item[2]]))\n",
    "for item in vader_res_merkel:\n",
    "    vader_all_leaders.append(list([item[0],item[1],item[2]]))\n",
    "for item in vader_res_modi:\n",
    "    vader_all_leaders.append(list([item[0],item[1],item[2]]))\n",
    "\n",
    "vader_all_df=pd.DataFrame(vader_all_leaders)    \n",
    "vader_all_df\n",
    "vader_all_df=vader_all_df.rename(columns={vader_all_df.columns[0]: \"Leader\", vader_all_df.columns[1]: \"Mood\",vader_all_df.columns[2]: \"Percentage\"})\n",
    "print(vader_all_df)\n",
    "\n",
    "sns.set_context('paper')\n",
    "plt.figure(figsize=(16, 10))\n",
    "my_pal = ['#5f9e6e', '#b55d60', '#5975a4']\n",
    "# create plot\n",
    "sns.barplot(x = 'Leader', y = 'Percentage', hue = 'Mood', data = vader_all_df,\n",
    "            palette = my_pal,\n",
    "            capsize = 0.05,             \n",
    "            saturation = 8,             \n",
    "            errcolor = 'gray', errwidth = 2,  \n",
    "            ci = 'sd'   \n",
    "            )\n",
    "\n",
    "plt.title('Leader Sentiment Comparison - VANDER', fontdict = {'fontsize' : 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Network Analysis</h2>\n",
    "\n",
    "The objective is to pick the top 20 targetted tweets and hashtag and build an network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node=[]\n",
    "p = re.compile(\"@[A-Za-z0-9_]+\") \n",
    "for item in df_trump['tweet']:\n",
    "    res=p.findall(str(item))\n",
    "    node.append(p.findall(str(item)))\n",
    "node\n",
    "node_str=[]\n",
    "for item in node:\n",
    "    for x in item:\n",
    "        node_str.append(x)\n",
    "        \n",
    "node_df = pd.DataFrame(node_str)\n",
    "node_df.columns = ['target']\n",
    "node_df.insert(0, 'source', '@realDonaldTrump')\n",
    "#print(node_df)\n",
    "\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "n1 = \"select source,target,count(*) as cnt from node_df where target <> '@realDonaldTrump' group by 1,2 order by cnt desc limit 20;\"\n",
    "nx1 = pysqldf(n1)\n",
    "\n",
    "r1 = \"select * from node_df where target in (select target from nx1)\"\n",
    "r2 = pysqldf(r1)\n",
    "#nx2 = r2[['source','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(r2)\n",
    "print(nx.info(G))\n",
    "plt.figure(figsize = (15,13))\n",
    "plt.title(\"Top 20 targeted tweets by Trump\", fontdict = {'fontsize' : 20})\n",
    "nx.draw_networkx(G, node_color='lightblue',node_shape  = \"h\",edge_color = \"grey\")\n",
    "plt.axis('off')\n",
    "plt.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node=[]\n",
    "p = re.compile(\"#[A-Za-z0-9_]+\") \n",
    "for item in df_trump['tweet']:\n",
    "    res=p.findall(str(item))\n",
    "    node.append(p.findall(str(item)))\n",
    "node\n",
    "node_str=[]\n",
    "for item in node:\n",
    "    for x in item:\n",
    "        node_str.append(x)\n",
    "        \n",
    "node_df1 = pd.DataFrame(node_str)\n",
    "\n",
    "node_df1.columns = ['target']\n",
    "node_df1.insert(0, 'source', '#Trump')\n",
    "\n",
    "pysqldf = lambda q1: sqldf(q1, globals())\n",
    "\n",
    "n01 = \"select source,target,count(*) as cnt from node_df1 where target <> '@realDonaldTrump' group by 1,2 order by cnt desc limit 20;\"\n",
    "nx01 = pysqldf(n01)\n",
    "nx02 = nx01[['source','target']]\n",
    "\n",
    "#print(nx02)\n",
    "G1 = nx.from_pandas_edgelist(nx02)\n",
    "print(nx.info(G1))\n",
    "plt.figure(figsize = (15,13))\n",
    "plt.title(\"Top 20 #hastag by Trump\", fontdict = {'fontsize' : 20})\n",
    "nx.draw_networkx(G1, node_color='lightblue',node_shape  = \"h\",edge_color = \"grey\")\n",
    "plt.axis('off')\n",
    "plt.show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conclusion</h3>\n",
    "<ol>\n",
    "  <li>Random Forest Model Classifier performed better on predicting the trump sentiments</li>\n",
    "    <li>There is an marginal difference between VADER and TEXTBLOB</li>\n",
    "    <li>Boris Johnson,Donald Trump and Narendra Modi are mostly postive</li>\n",
    "    <li>Donald Trump and Angela Merkel are top 2 postion in negative tweets</li>\n",
    "    <li>Based on Network Analysis, Trump targetted Media and fedral associates</li>\n",
    "    <li>In the recent times, Trump tweets are mostly words to Medical and Political hastags</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
